1) Нейроны "inputs" не входят ни в 1 из слоев сети и матрицу смежности "net.layerConnect"
Они рассматриваются как отдельный линейный слой.


Возможные решения :
    1) явно разбивать матрицу смежности на inputs, layers.
    2) Ввести дополнительные ЛИНЕЙНЫЕ нейроны в layers (1 input => 1 layer), "заморозить" связи "net.IW"

============================================================================================================


2) Функция initnw работает только с сигмоидальными нейронами, имеющими активационные функции :
    * tansig
    * logsig
    * purelin
    * poslin
    * radbas
    * satlin
    * satlins
    * softmax
    * tribas
Наличие bias не обязательно. Возможно использование различных комбинаций функций (из списка, указанного выше). 


Не работает, если есть хотя бы 1 из слоев имеет активационную функцию:
    * hardlim
    * hardlims
============================================================================================================








